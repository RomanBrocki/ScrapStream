#from selenium import webdriver
import os
#from selenium.webdriver.chrome.service import Service
#from selenium.webdriver.chrome.options import Options
import undetected_chromedriver as uc
import random
from selenium.webdriver.common.by import By  # Importando 'By' para usar os seletores
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from docx import Document
from pattern import pattern, replacement  # Importando pattern e replacement para substitui√ß√£o no texto
from tools.desfaz_censura import desfazer_censura

import time
from config import config  # Importando o arquivo de configura√ß√£o para os seletores

class NovelScraper:
    """
    A classe NovelScraper √© respons√°vel por realizar o scraping de cap√≠tulos de uma web novel,
    extrair o conte√∫do da p√°gina e gerar um eBook no formato .docx.
    
    Funcionalidades principais:
    - Scraping de cap√≠tulos: Realiza a extra√ß√£o de conte√∫do da web novel.
    - Substitui√ß√£o de padr√µes: Substitui padr√µes no texto do cap√≠tulo, conforme especificado no arquivo 'pattern.py'.
    - Gera√ß√£o de eBook: Compila o conte√∫do extra√≠do em um arquivo .docx.
    - Registro de ocorr√™ncias: Mant√©m um log das ocorr√™ncias de elementos espec√≠ficos no conte√∫do.
    """
    
    def __init__(self, config=config):
        """
        Inicializa a classe NovelScraper com um dicion√°rio de configura√ß√£o.

        Par√¢metros:
        - config: Dicion√°rio contendo as configura√ß√µes dos seletores de elementos e outras informa√ß√µes, como o perfil do navegador.
        """
        self.config = config  # A configura√ß√£o agora √© passada diretamente para o m√©todo

        self.ebook_name = ""
        self.start_url = ""
        self.end_url = ""
        self.occurrence_list = []  # Lista para armazenar as ocorr√™ncias encontradas
        self.occurrences = 0  # Contagem total de ocorr√™ncias
        self.save_path = ""  # Caminho onde o eBook ser√° salvo

    def scrape_chapters(self, start_url, end_url):
        """
        Realiza o scraping dos cap√≠tulos da web novel, extrai o conte√∫do e cria um eBook no formato .docx.

        Par√¢metros:
        - start_url: URL do primeiro cap√≠tulo da novel.
        - end_url: URL do √∫ltimo cap√≠tulo da novel.
        
        O m√©todo percorre os cap√≠tulos da novel, extrai o conte√∫do e cria um arquivo .docx com o texto extra√≠do.
        Ele tamb√©m realiza a substitui√ß√£o de padr√µes definidos no arquivo 'pattern.py'.
        """
        self.start_url = start_url
        self.end_url = end_url

        #options = Options()
        #options.add_argument("user-data-dir=" + self.config['profile'])  # Usando o perfil configurado
        #browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        options = uc.ChromeOptions()
        options.add_argument(f"--user-data-dir={self.config['profile']}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-blink-features=AutomationControlled")

        browser = uc.Chrome(options=options, headless=False)    

        document = Document()

        # Alterando o autor diretamente no documento, usando o valor de 'author' presente no config
        core_props = document.core_properties
        core_props.author = self.config['author']  # Define o autor

        # Lista para armazenar os t√≠tulos dos cap√≠tulos
        chapter_titles = []

        capitulo_count = 0             # Contador de cap√≠tulos
        falhas_recentes = 0            # Contador de falhas consecutivas
        modo_seguro = True             # Come√ßa em modo seguro, com delays mais longos

        # come√ßo do marcador de tempo
        inicio = time.time()
        
        while True:
            browser.get(self.start_url)

            # ‚è≥ Aguardar tempo aleat√≥rio para simular leitura real ap√≥s carregar a p√°gina
            time.sleep(random.uniform(1.2, 2.2))

            # üñ±Ô∏è Scroll gradual para simular comportamento humano
            browser.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.3);")
            time.sleep(random.uniform(0.4, 0.7))
            browser.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.7);")
            time.sleep(random.uniform(0.4, 0.7))
            browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")

            try:
                # Espera at√© que o t√≠tulo do cap√≠tulo e o conte√∫do estejam carregados na p√°gina
                WebDriverWait(browser, 20).until(EC.presence_of_element_located(self.config['chapter_title_selector']))
                WebDriverWait(browser, 20).until(EC.presence_of_element_located(self.config['chapter_content_selector']))

                # Extrai o t√≠tulo e o conte√∫do do cap√≠tulo
                chapter_title = browser.find_element(*self.config['chapter_title_selector']).text
                chapter_content = browser.find_element(*self.config['chapter_content_selector']).text
                
                # Remove censura por pontos
                chapter_content = desfazer_censura(chapter_content)


                # ‚úÖ Sucesso: reseta contador de falhas
                falhas_recentes = 0

            except Exception as e:
                print(f"Error loading page or elements not found. Retrying... Error: {e}")
                falhas_recentes += 1
                if falhas_recentes >= 2:
                    modo_seguro = True  # Ativa modo seguro ap√≥s 2 falhas seguidas
                time.sleep(5)
                continue  # Tenta novamente caso o erro ocorra

            # Verifica se o cap√≠tulo j√° foi copiado para evitar duplicatas
            capitulo_duplicado = chapter_title in chapter_titles

            if capitulo_duplicado:
                print(f"‚ö†Ô∏è Cap√≠tulo repetido detectado: '{chapter_title}'. Conte√∫do ser√° ignorado, mas avan√ßando normalmente.")
            else:
                # Adiciona o t√≠tulo do cap√≠tulo √† lista de cap√≠tulos
                chapter_titles.append(chapter_title)

                # Substitui os padr√µes definidos em `pattern.py`
                for element in pattern:
                    if element in chapter_content:
                        self.occurrence_list.append(f'Found: {element}')  # Registra a ocorr√™ncia
                        self.occurrences += 1  # Incrementa o contador de ocorr√™ncias
                    chapter_content = chapter_content.replace(element, replacement)  # Substitui o padr√£o encontrado

                # Adiciona o cap√≠tulo ao documento
                document.add_paragraph(f'{chapter_title}', style='Heading 2')  # T√≠tulo do cap√≠tulo
                document.add_paragraph(chapter_content)  # Conte√∫do do cap√≠tulo
                document.add_page_break()


            if self.start_url == self.end_url:
                break  # Se o scraping chegou ao √∫ltimo cap√≠tulo, sai do loop

            try:
                # Espera at√© que o bot√£o de pr√≥ximo cap√≠tulo esteja dispon√≠vel e clica nele
                WebDriverWait(browser, 20).until(EC.presence_of_element_located(self.config['next_chapter_selector']))
                browser.find_element(*self.config['next_chapter_selector']).click()
            except Exception as e:
                print(f"Next chapter button not found or error clicking next: {e}")
                break  # Se n√£o encontrar o pr√≥ximo cap√≠tulo ou n√£o conseguir clicar, interrompe o processo

            # Atualiza a URL para o pr√≥ximo cap√≠tulo
            self.start_url = browser.current_url

            # üîÅ Pausa adaptativa entre cap√≠tulos
            capitulo_count += 1

            if modo_seguro:
                if capitulo_count % 5 == 0:
                    time.sleep(random.uniform(5.0, 8.0))  # Pausa maior no modo seguro
                else:
                    time.sleep(random.uniform(1.0, 2.0))  # Pausa padr√£o no modo seguro

                # üß† Se passou 20 cap√≠tulos sem erro, volta ao modo r√°pido
                if falhas_recentes == 0 and capitulo_count % 20 == 0:
                    print("‚úÖ Est√°vel h√° 20 cap√≠tulos. Retornando ao modo r√°pido.")
                    modo_seguro = False
            else:
                if capitulo_count % 5 == 0:
                    time.sleep(random.uniform(2.0, 3.0))  # Pausa leve no modo r√°pido
                else:
                    time.sleep(random.uniform(0.4, 1.0))  # Pausa curta no modo r√°pido

        # --- REMOVIDO: √≠ndice manual (texto simples) que n√£o √© funcional no Kindle ---
        # Este trecho adicionava um √≠ndice de cap√≠tulos ao final do documento como par√°grafos normais.
        # Como o Calibre usa os estilos (Heading 2, etc.) para gerar um √≠ndice clic√°vel, 
        # este bloco n√£o √© mais necess√°rio e pode ser desativado.
        
        # Inserir o √≠ndice de cap√≠tulos no in√≠cio do documento
        # document.add_paragraph('Table of Contents', style='Heading 1')  # T√≠tulo do √≠ndice
        # for title in chapter_titles:
        #     document.add_paragraph(f'{title}', style='Normal')  # Adiciona cada cap√≠tulo ao √≠ndice

        # Salva o documento somente ap√≥s todos os cap√≠tulos e o √≠ndice terem sido adicionados
        
        # Calculo da marca√ß√£o de tempo
        fim = time.time()
        duracao_segundos = fim - inicio
        duracao_minutos = duracao_segundos / 60
        media_por_capitulo = duracao_segundos / capitulo_count if capitulo_count else 0

        print(f"üìò Scraping finalizado.")
        print(f"‚è±Ô∏è Dura√ß√£o total: {duracao_minutos:.2f} minutos")
        print(f"üìÑ Cap√≠tulos raspados: {capitulo_count}")
        print(f"üïí M√©dia por cap√≠tulo: {media_por_capitulo:.2f} segundos")
        


        document.save(os.path.join(self.save_path, "Novel.docx"))

        browser.quit()

        return {
            "duracao_min": round(duracao_minutos, 2),
            "capitulos": capitulo_count,
            "media_seg": round(media_por_capitulo, 2)
        }



    def get_log(self):
        """
        Retorna um log com todas as ocorr√™ncias encontradas durante o scraping.
        
        O log inclui todos os padr√µes encontrados no conte√∫do e o n√∫mero total de ocorr√™ncias.
        """
        log_list = ['List of found occurrences:']
        for occurrence in self.occurrence_list:
            log_list.append(occurrence)  # Adiciona cada ocorr√™ncia ao log
        log_list.append(f'Total occurrences found: {self.occurrences}')  # Adiciona o total de ocorr√™ncias
        return log_list

    def save_ebook(self, save_path, ebook_name):
        """
        Renomeia e salva o arquivo .docx com o nome do eBook fornecido.

        Par√¢metros:
        - save_path: Caminho onde o eBook ser√° salvo.
        - ebook_name: Nome do eBook a ser atribu√≠do ao arquivo .docx.
        """
        self.ebook_name = ebook_name
        self.save_path = save_path
        os.rename(os.path.join(self.save_path, "Novel.docx"), os.path.join(self.save_path, f"{self.ebook_name}.docx"))

    def run_scraper(self, ebook_name, save_path, start_url, end_url):
        """
        Executa o processo completo de scraping e salva o eBook gerado.

        Par√¢metros:
        - ebook_name: Nome do eBook a ser gerado.
        - save_path: Caminho onde o eBook ser√° salvo.
        - start_url: URL do primeiro cap√≠tulo.
        - end_url: URL do √∫ltimo cap√≠tulo.
        """
        self.save_path = save_path
        stats = self.scrape_chapters(start_url, end_url)  # Realiza o scraping dos cap√≠tulos e pega o retorno da mensura√ß√£o de tempo
        self.save_ebook(save_path, ebook_name)  # Salva o eBook gerado
        return stats













