{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae19ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# üì¶ IMPORTA√á√ïES GERAIS\n",
    "# ========================\n",
    "import os\n",
    "import re\n",
    "import importlib.util\n",
    "\n",
    "# ============================\n",
    "# üìÑ MANIPULA√á√ÉO DE DOCUMENTOS\n",
    "# ============================\n",
    "from docx import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ef7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos base dos arquivos a tratar\n",
    "ARQUIVO_ORIGINAL = r\"F:\\OneDrive\\Documentos\\Github\\ScrapStream\\tools\\Divine Emperor of Death 1801-1800_revisado.docx\"\n",
    "ARQUIVO_PATTERN = os.path.abspath(os.path.join(\"..\", \"pattern.py\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54d1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_capitulos_duplicados(caminho_entrada, caminho_saida, caminho_log=\"log_duplicados.txt\"):\n",
    "    doc = Document(caminho_entrada)\n",
    "    novo_doc = Document()\n",
    "\n",
    "    titulos_vistos = set()\n",
    "    capitulos_duplicados = []\n",
    "    pular_capitulo = False\n",
    "\n",
    "    for par in doc.paragraphs:\n",
    "        if par.style and hasattr(par.style, \"name\") and par.style.name == \"Heading 2\":\n",
    "            titulo = par.text.strip()\n",
    "            if titulo in titulos_vistos:\n",
    "                capitulos_duplicados.append(titulo)\n",
    "                print(f\"‚ö†Ô∏è Cap√≠tulo duplicado detectado: {titulo}\")\n",
    "                pular_capitulo = True\n",
    "                continue\n",
    "            else:\n",
    "                titulos_vistos.add(titulo)\n",
    "                novo_doc.add_paragraph(titulo, style=\"Heading 2\")\n",
    "                pular_capitulo = False\n",
    "        else:\n",
    "            if not pular_capitulo:\n",
    "                novo_doc.add_paragraph(par.text, style=par.style)\n",
    "\n",
    "    novo_doc.save(caminho_saida)\n",
    "    print(f\"‚úÖ Novo arquivo salvo como: {caminho_saida}\")\n",
    "\n",
    "    if capitulos_duplicados:\n",
    "        with open(caminho_log, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Cap√≠tulos duplicados removidos:\\n\\n\")\n",
    "            for titulo in capitulos_duplicados:\n",
    "                f.write(f\"{titulo}\\n\")\n",
    "        print(f\"üìÑ Log salvo como: {caminho_log}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhum cap√≠tulo duplicado encontrado.\")\n",
    "        \n",
    "    return capitulos_duplicados  # retorna lista de duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575cdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desfazer_censura(texto):\n",
    "    corrigidas = []\n",
    "\n",
    "    def juntar_letras(match):\n",
    "        original = match.group(0)\n",
    "        corrigido = original.replace('.', '')\n",
    "        if corrigido != original:\n",
    "            corrigidas.append(original)\n",
    "        return corrigido\n",
    "\n",
    "    texto_corrigido = re.sub(r'\\b(?:[a-zA-Z]\\.?){3,}\\b', juntar_letras, texto)\n",
    "    return texto_corrigido, corrigidas\n",
    "\n",
    "def limpar_censura_docx(caminho_docx):\n",
    "    doc = Document(caminho_docx)\n",
    "    novo_doc = Document()\n",
    "    log_corrigidas = []\n",
    "\n",
    "    for par in doc.paragraphs:\n",
    "        texto_corrigido, palavras = desfazer_censura(par.text)\n",
    "        log_corrigidas.extend(palavras)\n",
    "        novo_par = novo_doc.add_paragraph(texto_corrigido)\n",
    "        novo_par.style = par.style\n",
    "\n",
    "    nome_base, ext = os.path.splitext(caminho_docx)\n",
    "    caminho_saida = f\"{nome_base}_uncens{ext}\"\n",
    "    novo_doc.save(caminho_saida)\n",
    "\n",
    "    if log_corrigidas:\n",
    "        caminho_log = f\"{nome_base}_uncens_log.txt\"\n",
    "        with open(caminho_log, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Palavras censuradas corrigidas:\\n\\n\")\n",
    "            for palavra in sorted(set(log_corrigidas)):\n",
    "                f.write(f\"{palavra} ‚Üí {palavra.replace('.', '')}\\n\")\n",
    "        print(f\"üìù Log salvo em: {caminho_log}\")\n",
    "\n",
    "    return caminho_saida, log_corrigidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad98f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_or_remove_terms(pattern_path, docx_path, replace_with=None):\n",
    "    spec = importlib.util.spec_from_file_location(\"pattern_module\", pattern_path)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Erro ao carregar m√≥dulo: {pattern_path}\")\n",
    "    pattern_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(pattern_module)\n",
    "\n",
    "    terms_to_find = getattr(pattern_module, \"pattern\", [])\n",
    "    doc = Document(docx_path)\n",
    "\n",
    "    ocorrencias = []\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        for term in terms_to_find:\n",
    "            if term in paragraph.text:\n",
    "                paragraph.text = paragraph.text.replace(term, \"\" if replace_with is None else replace_with)\n",
    "                ocorrencias.append(term)\n",
    "\n",
    "    doc.save(docx_path)\n",
    "    return ocorrencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febc1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto_paragrafo(texto: str) -> str:\n",
    "    texto = texto.strip()\n",
    "    texto = re.sub(r\"(?im)^\\s*the\\s+end\\.?\\s*$\", \"\", texto)\n",
    "    texto = re.sub(r\"(?i)^ *(corrected|edited|fixes|changes|modifications):.*$\", \"\", texto, flags=re.MULTILINE)\n",
    "    texto = re.sub(r\"(?i)^ *[-‚Äì‚Ä¢*]?\\s*(removed|deleted|eliminated|cleaned)\\s+.*$\", \"\", texto, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    return texto.strip()\n",
    "\n",
    "\n",
    "def remover_artefatos_docx(caminho_arquivo: str) -> tuple[str, int]:\n",
    "    doc = Document(caminho_arquivo)\n",
    "    novo_doc = Document()\n",
    "\n",
    "    num_afetados = 0\n",
    "    for par in doc.paragraphs:\n",
    "        texto_original = par.text.strip()\n",
    "        texto_limpo = limpar_texto_paragrafo(texto_original)\n",
    "        if texto_original != texto_limpo:\n",
    "            num_afetados += 1\n",
    "        if texto_limpo:\n",
    "            novo_par = novo_doc.add_paragraph(texto_limpo)\n",
    "            novo_par.style = par.style\n",
    "\n",
    "\n",
    "    dir_base, nome = os.path.split(caminho_arquivo)\n",
    "    nome_base, ext = os.path.splitext(nome)\n",
    "    novo_nome = f\"{nome_base}_te{ext}\"\n",
    "    novo_caminho = os.path.join(dir_base, novo_nome)\n",
    "    novo_doc.save(novo_caminho)\n",
    "    return novo_caminho, num_afetados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a03154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_limpeza_completa(\n",
    "    caminho_original: str,\n",
    "    caminho_pattern_py: str = \"pattern.py\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Executa a limpeza completa de um .docx:\n",
    "    - Remove censura\n",
    "    - Remove artefatos LLM\n",
    "    - Remove marcas d‚Äô√°gua (pattern)\n",
    "    - Remove cap√≠tulos duplicados\n",
    "\n",
    "    Gera apenas um arquivo final com sufixo '_pipe.docx' e um log consolidado.\n",
    "    Remove todos os arquivos intermedi√°rios gerados.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Iniciando limpeza de: {caminho_original}\\n\")\n",
    "    log = []\n",
    "\n",
    "    # 1. Desfazer censura\n",
    "    caminho_1, censuradas = limpar_censura_docx(caminho_original)\n",
    "    log.append(f\"[üîì] Censura desfeita: {len(set(censuradas))} palavras corrigidas.\")\n",
    "\n",
    "    # 2. Limpar artefatos LLM\n",
    "    caminho_2, num_afetados = remover_artefatos_docx(caminho_1)\n",
    "    log.append(f\"[üßπ] Artefatos LLM removidos: {num_afetados} par√°grafos limpos.\")\n",
    "\n",
    "    # 3. Remover marcas d'√°gua\n",
    "    termos_removidos = replace_or_remove_terms(caminho_pattern_py, caminho_2)\n",
    "    log.append(f\"[üö´] Watermarks removidas: {len(set(termos_removidos))} termos.\")\n",
    "\n",
    "    # 4. Remover cap√≠tulos duplicados\n",
    "    caminho_final = os.path.splitext(caminho_original)[0] + \"_pipe.docx\"\n",
    "    duplicados = remover_capitulos_duplicados(caminho_2, caminho_final)\n",
    "    log.append(f\"[üìë] Cap√≠tulos duplicados removidos: {len(duplicados)}.\")\n",
    "\n",
    "    # 5. Limpeza dos intermedi√°rios\n",
    "    for f in [caminho_1, caminho_2]:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[‚ö†Ô∏è] Erro ao apagar arquivo intermedi√°rio {f}: {e}\")\n",
    "\n",
    "    # 6. Log final\n",
    "    caminho_log = os.path.splitext(caminho_final)[0] + \"_log.txt\"\n",
    "    with open(caminho_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"üìã Log de limpeza: {os.path.basename(caminho_original)}\\n\\n\")\n",
    "        for entrada in log:\n",
    "            f.write(entrada + \"\\n\")\n",
    "        if duplicados:\n",
    "            f.write(\"\\n--- Cap√≠tulos Duplicados ---\\n\")\n",
    "            for cap in duplicados:\n",
    "                f.write(f\"- {cap}\\n\")\n",
    "        if censuradas:\n",
    "            f.write(\"\\n--- Palavras Censuradas Corrigidas ---\\n\")\n",
    "            for palavra in sorted(set(censuradas)):\n",
    "                f.write(f\"{palavra} ‚Üí {palavra.replace('.', '')}\\n\")\n",
    "        if termos_removidos:\n",
    "            f.write(\"\\n--- Watermarks Removidas ---\\n\")\n",
    "            for termo in sorted(set(termos_removidos)):\n",
    "                f.write(f\"- {termo}\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Arquivo final salvo como:\\n{caminho_final}\")\n",
    "    print(f\"üìù Log salvo como:\\n{caminho_log}\")\n",
    "    return caminho_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab1e68f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Iniciando limpeza de: F:\\OneDrive\\Documentos\\Github\\ScrapStream\\tools\\Divine Emperor of Death 1701-1800_revisado.docx\n",
      "\n",
      "‚úÖ Novo arquivo salvo como: F:\\OneDrive\\Documentos\\Github\\ScrapStream\\tools\\Divine Emperor of Death 1701-1800_revisado_pipe.docx\n",
      "‚úÖ Nenhum cap√≠tulo duplicado encontrado.\n",
      "\n",
      "‚úÖ Arquivo final salvo como:\n",
      "F:\\OneDrive\\Documentos\\Github\\ScrapStream\\tools\\Divine Emperor of Death 1701-1800_revisado_pipe.docx\n",
      "üìù Log salvo como:\n",
      "F:\\OneDrive\\Documentos\\Github\\ScrapStream\\tools\\Divine Emperor of Death 1701-1800_revisado_pipe_log.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'F:\\\\OneDrive\\\\Documentos\\\\Github\\\\ScrapStream\\\\tools\\\\Divine Emperor of Death 1701-1800_revisado_pipe.docx'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executar_limpeza_completa(ARQUIVO_ORIGINAL, ARQUIVO_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61773c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70b085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
